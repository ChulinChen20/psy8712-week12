---
title: "week12"
author: "Chulin Chen"
date: "2023-04-24"
output: html_document
---

```{r setup, include=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(httr)
library(jsonlite)
library(lubridate)
library(tm)
library(qdap)
library(tidytext)
library(textstem)
```


```{r}
# write function to get post info within last year, use loops to scroll down to next pages

# getAllPosts <- function() {
#     url <- "https://www.reddit.com/r/IOPsychology/.json?q=timestamp%3A1641016800..1672552800&limit=100"
#     extract <- fromJSON(url)
#       posts <- extract$data$children$data %>% select(name,ups,title,created) %>%
#           mutate(date=as_datetime(created))
#     after <- posts[nrow(posts),1]
#     url.next <- paste0("https://www.reddit.com/r/IOPsychology/.json?q=timestamp%3A1641016800..1672552800&after=",after,"&limit=100")
#     extract.next <- fromJSON(url.next)
#     posts.next <- extract.next$data$children$data
# 
# # execute while loop as long as there are any rows in the data frame
#     while (!is.null(nrow(posts.next))) {
#         posts.next <- posts.next %>% select(name,ups,title,created) %>%
#           mutate(date=as_datetime(created))
#         posts <- rbind(posts, posts.next)
#         after <- posts[nrow(posts),1]
#         url.next <- paste0("https://www.reddit.com/r/IOPsychology/.json?q=timestamp%3A1641016800..1672552800&after=",after,"&limit=100")
#         Sys.sleep(3)
#         extract <- fromJSON(url.next)
#         posts.next <- extract$data$children$data
#     }
#     
#     return(posts)
# }
# 
# posts <- getAllPosts() 
# 
# # extract and rename required variables
# week12_tbl<- posts %>%
#   select(ups,title) %>%
#   rename(upvotes=ups)
# 
# # save data
# write.table(week12_tbl,"../data/week12.tbl")
```

```{r}
#create corpus
week12_tbl<- read.table("../data/week12.tbl")
io_corpus_original<- VCorpus(VectorSource(week12_tbl$title))

# replace abbreviation, contraction, lemmatization and convert to lowercase so that the same words in different forms can be treted as the same variable
# remove numbers, punctuation, stopwords, and whitespace as they are not important for the prediction
# the order of most processes does not matter a lot, 
# except that replace abbreviation, contraction should come before stopword removal and lemmatization for words to be recognizable for the later two processes
# and that remove 'io psychology' should come before removing punctuation so that "r" and "iopsychology" won't be combined
io_corpus_1 <- io_corpus_original %>%
  tm_map(content_transformer(replace_abbreviation)) %>%
  tm_map(content_transformer(replace_contraction)) %>%
  tm_map(content_transformer(str_to_lower)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, c("io psychology", "iopsy", "iopsychology")) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, stopwords("en")) %>%
  tm_map(stripWhitespace) 

# extract content for lemmatization and then restore the VCorpous
a<- list()
for (i in seq_along(io_corpus_1)) {
    a[i] <- lemmatize_strings(gettext(io_corpus_1[[i]][[1]]))
}
io_corpus <- VCorpus(VectorSource(unlist(a))) 

# compare corpus
compare_them <- function(x,y) {
  print(x$content[[6]]$content)
  print(y$content[[6]]$content)
}

compare_them(io_corpus_original, io_corpus)

tm_map(content_transformer(lemmatize_strings(io_corpus_original)))
```

