---
title: "week12"
author: "Chulin Chen"
date: "2023-04-24"
output: html_document
---

```{r setup, include=FALSE}
# Script Settings and Resources
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(httr)
library(jsonlite)
library(lubridate)
library(tm)
library(qdap)
library(tidytext)
library(textstem)
library(RWeka)
library(topicmodels)
library(wordcloud)
```


```{r}
## Download data
## write function to get post info within last year, use loops to scroll down to next pages

# getAllPosts <- function() {
#     url <- "https://www.reddit.com/r/IOPsychology/.json?q=timestamp%3A1641016800..1672552800&limit=100"
#     extract <- fromJSON(url)
#       posts <- extract$data$children$data %>% select(name,ups,title,created) %>%
#           mutate(date=as_datetime(created))
#     after <- posts[nrow(posts),1]
#     url.next <- paste0("https://www.reddit.com/r/IOPsychology/.json?q=timestamp%3A1641016800..1672552800&after=",after,"&limit=100")
#     extract.next <- fromJSON(url.next)
#     posts.next <- extract.next$data$children$data
# 
## execute while loop as long as there are any rows in the data frame
#     while (!is.null(nrow(posts.next))) {
#         posts.next <- posts.next %>% select(name,ups,title,created) %>%
#           mutate(date=as_datetime(created))
#         posts <- rbind(posts, posts.next)
#         after <- posts[nrow(posts),1]
#         url.next <- paste0("https://www.reddit.com/r/IOPsychology/.json?q=timestamp%3A1641016800..1672552800&after=",after,"&limit=100")
#         Sys.sleep(3)
#         extract <- fromJSON(url.next)
#         posts.next <- extract$data$children$data
#     }
#     
#     return(posts)
# }
# 
# posts <- getAllPosts() 
# 
## extract and rename required variables
# week12_tbl<- posts %>%
#   select(ups,title) %>%
#   rename(upvotes=ups)
# 
## save data
# write.table(week12_tbl,"../data/week12.tbl")
```

```{r}
# Data Import and Cleaning
#create corpus
week12_tbl<- read.table("../data/week12.tbl")
io_corpus_original<- VCorpus(VectorSource(week12_tbl$title))

# replace abbreviation, contraction, lemmatization and convert to lowercase so that the same words in different forms can be treted as the same variable.
# remove numbers, punctuation,and whitespace as they are not important for the prediction.
# the order of most processes does not matter a lot, 
# except that replace contraction should come before lemmatization for words to be recognizable.
# and that remove 'io psychology' should come after removing punctuation so that "i/o" can be captured
io_corpus_1 <- io_corpus_original %>%
  tm_map(content_transformer(str_to_lower)) %>%
  tm_map(removePunctuation)   %>%
  tm_map(removeWords, c("io psychology", "iopsy", "iopsych", "io psych", "iopsychology", "riopsychology", "organizational psychology", "industrial organizational psychology")) %>%   tm_map(content_transformer(replace_contraction))  %>%
  tm_map(stripWhitespace)

# extract content for lemmatization and then restore the VCorpous
a<- list()
for (i in seq_along(io_corpus_1)) {
    a[i] <- lemmatize_strings(gettext(io_corpus_1[[i]][[1]]))
}
io_corpus <- VCorpus(VectorSource(unlist(a))) 


# compare corpus
compare_them <- function(x,y) {
  print(x$content[[6]]$content)
  print(y$content[[6]]$content)
}

compare_them(io_corpus_original, io_corpus)
```

```{r}
# create bigram dtm
myTokenizer <- function(x) { 
  NGramTokenizer(x, Weka_control(min=2, max=2)) 
}

DTM <- DocumentTermMatrix(
  io_corpus,
  control = list(tokenize = myTokenizer))

io_dtm <-  as_tibble(as.matrix((DTM)))

# get n/k ratio (0.2477809)
977/3943

# remove sparse terms.
DTM_slim <- DTM %>%
  removeSparseTerms(.997)
io_slim_dtm_1 <- rowid_to_column(as_tibble(as.matrix((DTM_slim)))) 

# new n/k ratio (2.978659)
977/328

# remove empty entries

rowTotals <- apply(io_slim_dtm_1[,2:328], 1, sum)
table(rowTotals)
io_slim_dtm_a <- io_slim_dtm_1[rowTotals> 0, ] 
# slim dtm id
doc_id<- io_slim_dtm_a$rowid
# slim dtm matrix
io_slim_dtm<-io_slim_dtm_a[,2:328]
```

```{r}
# Analysis
lda_out <- LDA(
  io_slim_dtm,
  k = 2,
  method = "Gibbs",
  control = list(seed = 42)
)

# Tidy the matrix of word probabilities
lda_topics <- lda_out %>% 
  tidy(matrix = "beta")

# Arrange the topics by word probabilities in descending order
lda_topics %>% 
  arrange(desc(beta))

# word probability 
lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n=20)

# extract gamma metrix
doc_topic <- lda_out %>% 
  tidy(matrix = "gamma")

lda_matrix<- as.data.frame(cbind(doc_id,doc_topic))

week12_tbl_a<- as.data.frame(rowid_to_column(week12_tbl) )

topics_tbl_1<- left_join(week12_tbl_a,lda_matrix,by=c("rowid"="doc_id"))

topic_tbl <- topics_tbl_1 %>%
  select(-upvotes,-document) %>%
  rename(doc_id = rowid, original = title, probability = gamma)

# wordcloud
wordCounts <- colSums(io_dtm)
wordNames <- names(wordCounts)                          
wordcloud(wordNames, wordCounts,max.words =50,min.freq=3,scale=c(2.5,.5))

final_tbl <- topics_tbl_1 %>%
  select(-document) %>%
  rename(doc_id = rowid, original = title, probability = gamma)
```

(1) I think the two topics do not differ a lot from each other. Topic one is more related to io career whilst topic 2 is more related to discussion.
(2) Yes, the original posts seem to aligns with my interpretation. This should be face validity.
